{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RoBERTa Training â€“ Class Weighted Approach\n",
        "\n",
        "This notebook contains:\n",
        "* Data preprocessing\n",
        "* Class imbalance handling\n",
        "* Model training\n",
        "* Evaluation metrics\n",
        "* Confusion matrix"
      ],
      "metadata": {
        "id": "V03GPX4WeKc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6qwGHkXzZsS"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate scikit-learn evaluate\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "from google.colab import files\n",
        "\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"clean_amazon_reviews.csv\")\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "df = df[['clean_review', 'sentiment']].dropna().reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nCleaned dataset shape: {df.shape}\")\n",
        "print(\"\\nOriginal sentiment distribution:\")\n",
        "sentiment_counts = df['sentiment'].value_counts()\n",
        "print(sentiment_counts)\n",
        "print(f\"Class distribution percentages:\")\n",
        "for label, count in sentiment_counts.items():\n",
        "    print(f\"  {label}: {count} ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"sentiment\"])\n",
        "\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(f\"\\nLabel mapping: {label_mapping}\")\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(df[\"label\"]),\n",
        "    y=df[\"label\"]\n",
        ")\n",
        "\n",
        "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "print(f\"\\nCalculated class weights: {class_weight_dict}\")\n",
        "\n",
        "class_weights_tensor = torch.FloatTensor(class_weights)\n",
        "\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"clean_review\"].tolist(),\n",
        "    df[\"label\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[\"label\"]\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain samples: {len(train_texts)}\")\n",
        "print(f\"Validation samples: {len(val_texts)}\")\n",
        "\n",
        "train_dist = pd.Series(train_labels).value_counts().sort_index()\n",
        "val_dist = pd.Series(val_labels).value_counts().sort_index()\n",
        "print(f\"\\nTrain distribution: {train_dist.tolist()}\")\n",
        "print(f\"Validation distribution: {val_dist.tolist()}\")\n",
        "\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(texts, max_length=320):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "train_encodings = tokenize_function(train_texts)\n",
        "val_encodings = tokenize_function(val_texts)\n",
        "\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": train_encodings[\"input_ids\"],\n",
        "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
        "    \"labels\": train_labels\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": val_encodings[\"input_ids\"],\n",
        "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
        "    \"labels\": val_labels\n",
        "})\n",
        "\n",
        "print(\"âœ… Datasets created successfully!\")\n",
        "\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights.to(self.model.device)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        loss = loss_fn(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(le.classes_),\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "print(f\"âœ… Model loaded on: {device}\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
        "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
        "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
        "        \"recall\": recall_score(labels, preds, average=\"weighted\"),\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sentiment_results_weighted\",\n",
        "\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=6,\n",
        "\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "\n",
        "    gradient_accumulation_steps=1,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    seed=42,\n",
        "\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    class_weights=class_weights_tensor,\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting training with class weights...\")\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "print(\"\\nðŸ“Š Evaluating model...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Final evaluation metrics:\")\n",
        "for key, value in eval_results.items():\n",
        "    if key.startswith('eval_'):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "predictions = trainer.predict(val_dataset)\n",
        "logits = predictions.predictions\n",
        "labels = predictions.label_ids\n",
        "preds = np.argmax(logits, axis=1)\n",
        "\n",
        "y_true = le.inverse_transform(labels)\n",
        "y_pred = le.inverse_transform(preds)\n",
        "\n",
        "print(\"\\nðŸ“‹ Detailed Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=le.classes_)\n",
        "df_cm = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar_kws={'label': 'Count'})\n",
        "plt.title(\"Confusion Matrix - Class Weighted Model\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"\\nðŸ’¾ Saving model...\")\n",
        "model_save_path = \"roberta_sentiment_weighted\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "joblib.dump(le, f\"{model_save_path}/label_encoder.pkl\")\n",
        "\n",
        "metadata = {\n",
        "    'class_weights': class_weight_dict,\n",
        "    'label_mapping': label_mapping,\n",
        "    'model_name': model_name,\n",
        "    'num_classes': len(le.classes_),\n",
        "    'classes': le.classes_.tolist()\n",
        "}\n",
        "joblib.dump(metadata, f\"{model_save_path}/model_metadata.pkl\")\n",
        "\n",
        "print(\"âœ… Model saved successfully!\")\n",
        "\n",
        "!zip -r roberta_sentiment_weighted.zip roberta_sentiment_weighted\n",
        "\n",
        "print(\"ðŸ“¥ Downloading model...\")\n",
        "files.download(\"roberta_sentiment_weighted.zip\")\n",
        "\n",
        "\n",
        "class SentimentPredictor:\n",
        "    def __init__(self, model_path=\"roberta_sentiment_weighted\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "        self.label_encoder = joblib.load(f\"{model_path}/label_encoder.pkl\")\n",
        "        self.metadata = joblib.load(f\"{model_path}/model_metadata.pkl\")\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        print(f\"âœ… Sentiment predictor loaded on: {self.device}\")\n",
        "\n",
        "    def predict(self, texts, batch_size=32):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "            encodings = self.tokenizer(\n",
        "                batch_texts,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=256,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            encodings = {k: v.to(self.device) for k, v in encodings.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**encodings)\n",
        "                logits = outputs.logits\n",
        "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "            preds = np.argmax(probs, axis=1)\n",
        "            labels = self.label_encoder.inverse_transform(preds)\n",
        "\n",
        "            for j, text in enumerate(batch_texts):\n",
        "                prob_dict = {\n",
        "                    label: float(probs[j][idx])\n",
        "                    for idx, label in enumerate(self.label_encoder.classes_)\n",
        "                }\n",
        "\n",
        "                results.append({\n",
        "                    \"text\": text,\n",
        "                    \"predicted_label\": labels[j],\n",
        "                    \"confidence\": float(np.max(probs[j])),\n",
        "                    \"probabilities\": prob_dict\n",
        "                })\n",
        "\n",
        "        return results[0] if len(texts) == 1 else results\n",
        "\n",
        "predictor = SentimentPredictor()\n",
        "\n",
        "\n",
        "print(\"\\nðŸ§ª Testing the trained model:\")\n",
        "\n",
        "test_cases = [\n",
        "    \"This product is amazing and works perfectly!\",\n",
        "    \"Worst purchase ever, totally disappointed\",\n",
        "    \"Quality is okay, nothing special\",\n",
        "    \"The quality was bad but delivery was okay\",\n",
        "    \"Excellent service and fast shipping\",\n",
        "    \"Not worth the money, poor quality\"\n",
        "]\n",
        "\n",
        "print(\"\\nSingle predictions:\")\n",
        "for text in test_cases:\n",
        "    result = predictor.predict(text)\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"Prediction: {result['predicted_label']} (confidence: {result['confidence']:.3f})\")\n",
        "    print(f\"Probabilities: {result['probabilities']}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nBatch prediction:\")\n",
        "batch_results = predictor.predict(test_cases)\n",
        "for result in batch_results:\n",
        "    print(f\"{result['predicted_label']}: {result['text']}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Complete! Your optimized class-weighted sentiment model is ready!\")"
      ]
    }
  ]
}